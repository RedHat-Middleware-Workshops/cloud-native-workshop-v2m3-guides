== Service Mesh and Identity

In this module, you will learn how to prevent cascading failures in a distributed environment, how to detect misbehaving services, and how to avoid having to implement resiliency and monitoring in your business logic. As we transition our applications towards a distributed architecture with microservices deployed across a distributed
network, many new challenges await us.

Technologies like containers and container orchestration platforms like OpenShift solve the deployment of our distributed
applications quite well, such as dealing with:

* Unpredictable failure modes
* Verifying end-to-end application correctness
* Unexpected system degradation
* Continuous topology changes
* The use of elastic/ephemeral/transient resources

Today, developers are responsible for taking into account these challenges, and do things like:

* Circuit breaking and Bulkheading (e.g. with Netfix Hystrix)
* Timeouts/retries
* Service discovery (e.g. with Eureka)
* Client-side load balancing (e.g. with Netfix Ribbon)

Another challenge is each runtime and language addresses these with different libraries and frameworks, and in
some cases there may be no implementation of a particular library for your chosen language or runtime.

In this section we'll explore how to use the OpenShift _Service Mesh_. Service Mesh is based on the _Istio_ open source project. It enables a much more robust, reliable, and resilient application in the face of the new world of dynamic distributed applications.

=== What is Istio?

'''''

image::istio-logo.png[Logo, 600, align="center"]

Istio is an open, platform-independent service mesh designed to manage
communications between microservices and applications in a transparent
way. It provides behavioral insights and operational control over the
service mesh as a whole. It provides a number of key capabilities
uniformly across a network of services:

* *Traffic Management* - Control the flow of traffic and API calls
between services, make calls more reliable, and make the network more
robust in the face of adverse conditions.
* *Observability* - Gain understanding of the dependencies between
services and the nature and flow of traffic between them, providing the
ability to quickly identify issues.
* *Policy Enforcement* - Apply organizational policy to the interaction
between services, ensure access policies are enforced and resources are
fairly distributed among consumers. Policy changes are made by
configuring the mesh, not by changing application code.
* *Service Identity and Security* - Provide services in the mesh with a
verifiable identity and provide the ability to protect service traffic
as it flows over networks of varying degrees of trustability.

These capabilities greatly decrease the coupling between application
code, the underlying platform, and policy. This decreased coupling not
only makes services easier to implement, but also makes it simpler for
operators to move application deployments between environments or to new
policy schemes. Applications become inherently more portable as a
result.

Sounds fun, right? Let’s get started!

=== Examine Istio

'''''

For this module we’ve already installed Istio into our OpenShift
platform as well as as serveral useful tools to use with it. Istio is
installed in the `istio-system` project, and we’ve also added a few
other commonly used tools like Kiali and Jaeger (more on these later)


_Kiali_ is an observability console for Istio with service mesh configuration capabilities. It helps you to understand the structure of your service mesh by inferring the topology, and also provides the health of your mesh. Kiali provides detailed metrics

_Jaeger_ inspired by Dapper and OpenZipkin, is a distributed tracing system released as open source by Uber Technologies. It is used for monitoring and troubleshooting microservices-based distributed systems.

You can also read a bit more about the
https://istio.io/docs[Istio]\{:target="_blank"} architecture below:

=== Istio Details

'''''

An Istio service mesh is logically split into a _data plane_ and a
_control plane_.

The *data plane* is composed of a set of intelligent proxies (_Envoy_
proxies) deployed as _sidecars_ to your application’s pods in OpenShift
that mediate and control all network communication between
microservices.

The *control plane* is responsible for managing and configuring proxies
to route traffic, as well as enforcing policies at runtime.

The following diagram shows the different components that make up each
plane:

image::arch.png[Istio Arch, 600, align="center"]

==== Istio Components

Istio uses an extended version of the
https://envoyproxy.github.io/envoy/[Envoy]\{:target="_blank"} proxy as a
_side car_ container attached to each service Pod. Envoy is a
high-performance proxy developed in C++ to mediate all inbound and
outbound traffic for all services in the service mesh. Istio leverages
Envoy’s many built-in features, for example:

* Dynamic service discovery
* Load balancing
* TLS termination
* HTTP/2 and gRPC proxies
* Circuit breakers
* Health checks
* Staged rollouts with %-based traffic split
* Fault injection
* Rich metrics

*Envoy* is the _data plane_ component that deployed as a _sidecar_ to
the relevant service in the same Kubernetes pod. This deployment allows
Istio to extract a wealth of signals about traffic behavior as
attributes. Istio can, in turn, use these attributes in _Mixer_ to
enforce policy decisions, and send them to monitoring systems to provide
information about the behavior of the entire mesh.

Mixer is the _control plane_ component responsible for enforcing access
control and usage policies across the service mesh, and collects
telemetry data from the Envoy proxy and other services. The proxy
extracts request level attributes, and sends them to Mixer for
evaluation.

Mixer includes a flexible plugin model. This model enables Istio to
interface with a variety of host environments and infrastructure
backends. Thus, Istio abstracts the Envoy proxy and Istio-managed
services from these details.

*Pilot* is the _control plane_ component responsible for configuring the
proxies at runtime. Pilot provides service discovery for the Envoy
sidecars, traffic management capabilities for intelligent routing (for
example, A/B tests or canary deployments), and resiliency (timeouts,
retries, and circuit breakers).

Pilot converts high level routing rules that control traffic behavior
into Envoy-specific configurations, and propagates them to the sidecars
at runtime. Pilot abstracts platform-specific service discovery
mechanisms and synthesizes them into a standard format that any sidecar
conforming with the https://github.com/envoyproxy/data-plane-api[Envoy
data plane APIs]\{:target="_blank"} can consume. This loose coupling
allows Istio to run on multiple environments such as Kubernetes, Consul,
or Nomad, while maintaining the same operator interface for traffic
management.

*Citadel* is the _control plane_ component responsible for certificate
issuance and rotation. Citadel provides strong service-to-service and
end-user authentication with built-in identity and credential
management. You can use Citadel to upgrade unencrypted traffic in the
service mesh. Using Citadel, operators can enforce policies based on
service identity rather than on network controls.

*Galley* is Istio’s configuration validation, ingestion, processing and
distribution component. It is responsible for insulating the rest of the
Istio components from the details of obtaining user configuration from
the underlying platform (e.g. Kubernetes).

Several *Add-ons* components are used to provide additional
visualizations, metrics, and tracing functions:

* https://www.kiali.io/[Kiali]\{:target="_blank"} - Service mesh
observability and configuration
* https://prometheus.io/[Prometheus]\{:target="_blank"} - Systems
monitoring and alerting toolkit
* https://grafana.com/[Grafana]\{:target="_blank"} - Allows you to
query, visualize, alert on and understand your metrics
* http://jaeger.readthedocs.io/[Jaeger Tracing]\{:target="_blank"} -
Distributed tracing to gather timing data needed to troubleshoot latency
problems in microservice architectures

We will use these in future steps in this scenario!

=== Getting Ready for the labs

==== If this is the first module you are doing today

You will be using Red Hat CodeReady Workspaces, an online IDE based on https://www.eclipse.org/che/[Eclipe Che, window=_blank]. *Changes to files are auto-saved every few seconds*, so you don’t need to explicitly
save changes.

To get started, {{ ECLIPSE_CHE_URL }}[access the Che instance, window=_blank] and log in using the username
and password you’ve been assigned (e.g. `{{ USER_ID }}/{{ CHE_USER_PASSWORD }}`):

image::che-login.png[cdw, 700]

Once you log in, you’ll be placed on your personal dashboard. Click on the name of
the pre-created workspace on the left, as shown below (the name will be different depending on your assigned number). You can also
click on the name of the workspace in the center, and then click on the green button that says _Open_ on the top right hand side
of the screen.

After a minute or two, you’ll be placed in the workspace:

image::che-workspace.png[cdw, 600, align="center"]

This IDE is based on Eclipse Che (which is in turn based on MicroSoft VS Code editor).

You can see icons on the left for navigating between project explorer, search, version control (e.g. Git), debugging, and other plugins.  You’ll use these during the course of this workshop. Feel free to click on them and see what they do:

image::crw-icons.png[cdw, 400, align="center"]

[NOTE]
====
If things get weird or your browser appears, you can simply reload the browser tab to refresh the view.
====

Many features of CodeReady Workspaces are accessed via *Commands*. You can see a few of the commands listed with links on the home page (e.g. _New File.._, _Git Clone.._, and others).

If you ever need to run commands that you don't see in a menu, you can press kbd:[F1] to open the command window, or the more traditional kbd:[Control+SHIFT+P] (or kbd:[Command+SHIFT+P] on Mac OS X).

Let's import our first project. Click on **Git Clone..** (or type kbd:[F1], enter 'git' and click on the auto-completed _Git Clone.._ )

image::che-workspace-gitclone.png[cdw, 600, align="center"]

Step through the prompts, using the following value for **Repository URL**:

[source,none,role="copypaste"]
----
https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-labs.git
----

image::crw-clone-repo.png[crw, 600, align="center"]

Next, select `/projects` in the drop-down menu for destination directory:

image::crw-clone-dest.png[crw, 600, align="center"]

And click *Select Repository Location*.

Once imported, choose **Add to existing workspace** when prompted.

The project is imported into your workspace and is visible in the project explorer:

image::crw-clone-explorer.png[crw, 600, align="center"]

[NOTE]
====
The Terminal window in CodeReady Workspaces. You can open a terminal window for any of the containers running in your Developer workspace. For the rest of these labs, anytime you need to run a command in a
terminal, you can use the **>_ New Terminal** command on the right:
====

image::codeready-workspace-terminal.png[codeready-workspace-terminal, 600, align="center"]
